{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b55221",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c91294",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"humaneval_tuned_prompts\"\n",
    "runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb04a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "def ask_question_with_openai(role_description, prompt):\n",
    "    prompt = f\"\"\"\n",
    "{role_description}\\n\n",
    "{prompt}\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [entry[\"prompt\"] for entry in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a65dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_description = \"You are a Python programming expert. Please solve the following problem using Python code. Please make sure to just answer with the function with the same signture as the given one without any additional text or comments.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2bf4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code_with_openai(role_description, prompt):\n",
    "    response = ask_question_with_openai(role_description, prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution_dataset(role_description, dataset):\n",
    "    solutions = []\n",
    "    for entry in dataset:\n",
    "        prompt = entry[\"prompt\"]\n",
    "        solution = generate_code_with_openai(role_description, prompt)\n",
    "        solutions.append(solution)\n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473fda1",
   "metadata": {},
   "source": [
    "## Solution dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for i in range(runs):\n",
    "    generated_solutions = generate_solution_dataset(role_description, prompts)\n",
    "\n",
    "    # Save the generated solutions to a JSON file\n",
    "    with open(rf\"../data/{dataset_name}_{i}_solutions.json\", \"w\") as f:\n",
    "        json.dump(generated_solutions, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efcbda",
   "metadata": {},
   "source": [
    "## Experiment evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2918e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "dataset = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "\n",
    "\n",
    "\n",
    "for run in range(runs):\n",
    "\n",
    "    # Load your generated solutions\n",
    "    with open(rf\"../data/{dataset_name}_{run}_solutions.json\", \"r\") as f:\n",
    "        loaded_solutions = json.load(f)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for i, row in enumerate(dataset):\n",
    "        prompt = row['prompt']  # This has imports, helpers, and the function signature (no impl)\n",
    "        solution = loaded_solutions[i]\n",
    "        # Remove code block markers if present\n",
    "        code = solution\n",
    "        if code.startswith('```'):\n",
    "            code = code.split('```')[1] if '```' in code else code\n",
    "        code = code.replace('python', '').strip('` \\n')\n",
    "        \n",
    "        # Combine prompt and solution (prompt includes function signature, solution may also include it, which is fine)\n",
    "        combined_code = prompt + \"\\n\" + code\n",
    "\n",
    "        test_code = row['test']\n",
    "        entry_point = row['entry_point']\n",
    "        task_id = row['task_id']\n",
    "        test_passed = False\n",
    "        error = None\n",
    "\n",
    "        try:\n",
    "            namespace = {}\n",
    "            exec(combined_code, namespace)         # Define helpers, imports, and main function\n",
    "            exec(test_code, namespace)             # Define 'check'\n",
    "            namespace['candidate'] = namespace[entry_point]  # Solution as candidate\n",
    "            namespace['check'](namespace['candidate'])        # <-- THIS RUNS THE TESTS\n",
    "            test_passed = True\n",
    "        except Exception as e:\n",
    "            error = str(e)\n",
    "            test_passed = False\n",
    "\n",
    "        results.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"test_passed\": test_passed,\n",
    "            \"error\": error,\n",
    "        })\n",
    "\n",
    "    out_file = rf\"../results/{dataset_name}_{i}_results.json\"\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"Finished run {run}. Results saved to\", out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c68ce",
   "metadata": {},
   "source": [
    "## Approximate Price Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "\n",
    "# Select the encoding for your model\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "with open(rf\"../data/{dataset_name}_solutions.json\", \"r\") as f:\n",
    "    loaded_solutions = json.load(f)\n",
    "\n",
    "# Your input prompt\n",
    "input = \", \".join(prompts)\n",
    "output = \", \".join(loaded_solutions)\n",
    "\n",
    "# Count tokens\n",
    "num_input_tokens = len(encoding.encode(input))\n",
    "num_output_tokens = len(encoding.encode(output))  # Assuming no output tokens for this example, adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d8713",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 10\n",
    "datasets = 5\n",
    "# Prices for GPT-4o\n",
    "price_per_million_input_tokens = 2.5\n",
    "price_per_million_output_tokens = 10\n",
    "\n",
    "print(f\"Estimated cost for {runs} runs with {datasets} datasets: {runs * datasets * (num_input_tokens * price_per_million_input_tokens + num_output_tokens * price_per_million_output_tokens) / 1_000_000}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
