{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77098e",
   "metadata": {},
   "source": [
    "## Load humaneval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "dataset = load_dataset(\"openai_humaneval\",split=\"test\")\n",
    "dataset = [entry for entry in dataset] # features: ['task_id', 'prompt', 'canonical_solution', 'test', 'entry_point']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [entry[\"prompt\"] for entry in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abe3e9",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93656885",
   "metadata": {},
   "source": [
    "#### Prompt test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a788f28",
   "metadata": {},
   "source": [
    "## Tuned prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b905a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_prompts = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            \"You are an expert prompt rewriter. Your task is to improve the clarity and helpfulness of docstrings \"\n",
    "            \"for code generation. Only rewrite the docstring (the text inside triple double quotes: \\\"\\\"\\\" ... \\\"\\\"\\\"). \"\n",
    "            \"Do not modify the function signature or write any implementation code.\"\n",
    "        )},\n",
    "        {\"role\": \"user\",   \"content\": f\"{prompt}\"},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    out_ids = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model.device),\n",
    "                            max_new_tokens=1024, do_sample=True)\n",
    "    \n",
    "    tuned_prompts.append(tokenizer.decode(out_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d510bc",
   "metadata": {},
   "source": [
    "### Dump results into json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = [{\"id\": i, \"prompt\": p} for i, p in enumerate(tuned_prompts)]\n",
    "\n",
    "with open(\"tuned_prompts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c71cab",
   "metadata": {},
   "source": [
    "### Extract response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(r\"\\[/INST\\](.+)\", re.DOTALL)\n",
    "docstrings = []\n",
    "for i, prompt in enumerate(tuned_prompts):\n",
    "    matches = pattern.findall(prompt)\n",
    "    if matches:\n",
    "        docstrings.append(matches[0].strip())\n",
    "    else:\n",
    "        print(\"No match found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d8469",
   "metadata": {},
   "source": [
    "### Standardize docstrings by removing quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c8fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstring_pattern = re.compile(r\"['\\\"]{3}(.+?)['\\\"]{3}\", re.DOTALL)\n",
    "\n",
    "docstrings_without_quotes = []\n",
    "for i, prompt in enumerate(docstrings):\n",
    "    if prompt.startswith('\"\"\"') and prompt.endswith('\"\"\"'): # Standard\n",
    "        # print(i)\n",
    "        docstrings_without_quotes.append(prompt[3:-3].strip())  # Remove the leading and trailing triple quotes and whitespace\n",
    "    elif prompt.startswith('\"\"\"'): # Missing end quotes\n",
    "        # print(i)\n",
    "        docstrings_without_quotes.append(prompt[3:].strip())\n",
    "    elif prompt.endswith('\"\"\"'): # Missing start quotes\n",
    "        # print(i)\n",
    "        docstrings_without_quotes.append(prompt[:-3].strip())\n",
    "    elif prompt.startswith(\"'''\") and prompt.endswith(\"'''\"): # Single quotes instead of double quotes\n",
    "        # print(i)\n",
    "        docstrings_without_quotes.append(prompt[3:-3].strip())\n",
    "    elif prompt.startswith(\"'''\"): # Missing end quotes with single quotes\n",
    "        # print(i)\n",
    "        docstrings_without_quotes.append(prompt[3:].strip())\n",
    "    elif prompt.endswith(\"'''\"): # Missing start quotes with single quotes\n",
    "        # print(i)\n",
    "        docstrings_without_quotes.append(prompt[:-3].strip())\n",
    "    elif i == 10:  # Special case for the 10th prompt\n",
    "        docstrings_10 = []\n",
    "        docstrings_10 = docstring_pattern.findall(prompt)\n",
    "        docstrings_10 = [docstring.strip() for docstring in docstrings_10]\n",
    "        if docstrings_10:\n",
    "            docstrings_without_quotes.append(docstrings_10)\n",
    "    elif prompt.startswith('```python'):\n",
    "        # Find tripple single or double quotes and append the content inbetween\n",
    "        match = docstring_pattern.search(prompt)\n",
    "        if match:\n",
    "            docstrings_without_quotes.append(match.group(1).strip())\n",
    "    else:\n",
    "        print(f\"Unexpected format in response {i}: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ee866",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docstrings), len(docstrings_without_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d12c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all prompts contain a docstring\n",
    "for i, prompt in enumerate(prompts):\n",
    "    if not '\"\"\"' in prompt and not \"'''\" in prompt:\n",
    "        print(f\"No docstring found in prompt {i}.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d21be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hardcopy of the dataset to modify\n",
    "import copy\n",
    "new_dataset = copy.deepcopy(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfad4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstrings_with_quotes = [f'\"\"\"\\n{docstring}\\n\"\"\"' for docstring in docstrings_without_quotes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstrings_with_quotes[10] = (f'{docstrings_without_quotes[10][0]}', f'\"\"\"{docstrings_without_quotes[10][1]}\"\"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_pattern = re.compile(r\"['\\\"]{3}(.+?)['\\\"]{3}\", re.DOTALL)\n",
    "\n",
    "for i, entry in enumerate(new_dataset):\n",
    "    # Go through each entry and replace the docstring in the prompt\n",
    "    if not '\"\"\"' in entry[\"prompt\"] and not \"'''\" in entry[\"prompt\"]:\n",
    "        print(f\"No docstring found in prompt {i}.\")\n",
    "        continue\n",
    "    \n",
    "    if entry[\"prompt\"].count('\"\"\"') == 2 or entry[\"prompt\"].count(\"'''\") == 2:\n",
    "        entry[\"prompt\"] = re.sub(replace_pattern, docstrings_with_quotes[i], entry[\"prompt\"], count=1)\n",
    "    else:\n",
    "        if i == 10: # Special case for the 10th prompt\n",
    "            entry[\"prompt\"] = re.sub(replace_pattern, f'\"\"\"{docstrings_with_quotes[i][1]}\"\"\"', entry[\"prompt\"], count=1)\n",
    "            entry[\"prompt\"] = re.sub(replace_pattern, f'\"\"\"{docstrings_with_quotes[i][0]}\"\"\"', entry[\"prompt\"], count=1)\n",
    "            print(f\"Verify special case for entry {i}:\\nOG:{dataset[i]['prompt']}\\nNew:{entry['prompt']}\")\n",
    "            continue\n",
    "        if entry[\"prompt\"].count('\"\"\"') == 4 or entry[\"prompt\"].count(\"'''\") == 4:\n",
    "            entry[\"prompt\"] = re.sub(replace_pattern, docstrings_with_quotes[i], entry[\"prompt\"], count=1)\n",
    "            print(f\"Verify special case for entry {i}:\\nOG:{dataset[i]['prompt']}\\nNew:{entry['prompt']}\")\n",
    "        print(f\"Unexpected format in prompt {i}: {entry['prompt']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_dataset), len(docstrings_without_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b3d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the modified dataset to a new JSON file\n",
    "import json\n",
    "\n",
    "with open(\"humaneval_tuned_prompts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_dataset, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the modified dataset to disk using datasets library\n",
    "from datasets import Dataset\n",
    "modified_dataset = Dataset.from_list(new_dataset)\n",
    "modified_dataset.save_to_disk(\"humaneval_tuned_prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba24836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modified dataset from disk\n",
    "from datasets import load_from_disk\n",
    "loaded_dataset = load_from_disk(\"humaneval_tuned_prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0\n",
    "print(dataset[id][\"prompt\"])\n",
    "print(new_dataset[id][\"prompt\"])\n",
    "print(loaded_dataset[id][\"prompt\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
